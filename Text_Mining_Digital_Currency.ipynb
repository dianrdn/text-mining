{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Mining - Digital Currency",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPf0MqbMwZz8BfNOhVK2Fyv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dianrdn/text-mining/blob/main/Text_Mining_Digital_Currency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-mrTVCfmbpX"
      },
      "source": [
        "## **Text Mining**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6v1llWX1ybs"
      },
      "source": [
        "### **Install and Load Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orcAloXKEtUi"
      },
      "source": [
        "# Install packages\n",
        "! pip install tweet-preprocessor\n",
        "! pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFNTlh7j0yW-"
      },
      "source": [
        "# Load packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import preprocessor as p\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import wordcloud\n",
        "import nltk\n",
        "import warnings\n",
        "import itertools\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WPn6T5IYLO"
      },
      "source": [
        "# Import module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
        "from tqdm import tqdm\n",
        "from nltk import bigrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-TGl07g1GR_"
      },
      "source": [
        "# Set parameter\n",
        "warnings.filterwarnings('ignore')\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POd_N1Vx18Ft"
      },
      "source": [
        "### **Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX4JNHdFmLbd"
      },
      "source": [
        "# Import data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/apriandito/digital-currency/main/data/tweet_digitalcurrency_text-sample.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9lNfxTM1wV0"
      },
      "source": [
        "# Lihat 5 baris pertama data\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfntSX4DopwQ"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIXInlqooGaO"
      },
      "source": [
        "### **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PtaspkU8ei5"
      },
      "source": [
        "# Pilih 5 kolom teks saja\n",
        "tweet = df[['text']]\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmpAtGBFFGCG"
      },
      "source": [
        "### **Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewjn6JMbDejZ"
      },
      "source": [
        "# Membuat fungsi transformasi tweet\n",
        "def transform_tweet(row):\n",
        "  tweet = row['text']\n",
        "  tweet = p.clean(tweet)\n",
        "  tweet = str.lower(tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMa_w71GEaFS"
      },
      "source": [
        "# Mengaplikasikan fungsi transofrmasi\n",
        "tweet['transformed'] = tweet.apply(transform_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ZiliHSFSnH"
      },
      "source": [
        "### **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0l9wvMPFpNE"
      },
      "source": [
        "# Download Punkt\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfyNe2VC6rN"
      },
      "source": [
        "# Membuat fungsi tokenization\n",
        "def tokenize_tweet(row):\n",
        "    tweet = row['transformed']\n",
        "    tokens = nltk.word_tokenize(tweet)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFmjyoGi_rxN"
      },
      "source": [
        "# Mengaplikasikan fungsi tokenization\n",
        "tweet['tokenized'] = tweet.apply(tokenize_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtKXiR7YF9RZ"
      },
      "source": [
        "### **Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNKhzkZvF7mR"
      },
      "source": [
        "# Download Wordnet\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXSeJu9gGK5e"
      },
      "source": [
        "# Membuat fungsi lemmatization\n",
        "def lemmatize_tweet(row):\n",
        "    list = row['tokenized']\n",
        "    lemmatize_list = [WordNetLemmatizer().lemmatize(w, pos= 'v') for w in list]\n",
        "    return(lemmatize_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZm2bfvaGjyu"
      },
      "source": [
        "# Mengaplikasikan fungsi lemmatization\n",
        "tweet['lemmatized'] = tweet.apply(lemmatize_tweet, axis=1)\n",
        "\n",
        "# Lihat 10 baris pertama data\n",
        "tweet.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zeuh1xNYIzy5"
      },
      "source": [
        "### **Stopword Removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7XcHI7VIwjW"
      },
      "source": [
        "# Download stopword bahasa inggris\n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words(\"english\"))     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUidpdOBuBU"
      },
      "source": [
        "# Membuat fungsi lemmatization\n",
        "def stopword_tweet(row):\n",
        "    list = row['lemmatized']\n",
        "    stopword_list = [w for w in list if not w in stops]\n",
        "    return(stopword_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPei0h4uCEh5"
      },
      "source": [
        "# Mengaplikasikan fungsi Stopword\n",
        "tweet['stopword'] = tweet.apply(stopword_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAkl9x-LI4HL"
      },
      "source": [
        "### **Rejoin Token**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGzqcONlCHYo"
      },
      "source": [
        "# Membuat fungsi rejoin untuk mengembalikan sebagai kalimat utuh\n",
        "def rejoin_tweet(row):\n",
        "    list = row['stopword']\n",
        "    joined_words = ( \" \".join(list))\n",
        "    return joined_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GkNjo6bCc36"
      },
      "source": [
        "# Mengaplikasikan fungsi rejoin\n",
        "tweet['final'] = tweet.apply(rejoin_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OkxLRK7JwRZ"
      },
      "source": [
        "# Final tweet yang sudah di proses\n",
        "tweet_clean = tweet[['final']]\n",
        "tweet_clean = tweet_clean.rename(columns={'final': 'text'})\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet_clean.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_YvObORoOm8"
      },
      "source": [
        "### **Wordcloud**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmiFeAwzoOK2"
      },
      "source": [
        "# Visualisasi Word Cloud\n",
        "text_wordcloud = \" \".join(tweet for tweet in tweet_clean.text)\n",
        "\n",
        "cloud = WordCloud(background_color='white').generate(text_wordcloud)\n",
        "\n",
        "plt.figure(figsize=(10, 10), facecolor=None)\n",
        "plt.imshow(cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VylHW2_Um_eo"
      },
      "source": [
        "### **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqUKHLCim63Q"
      },
      "source": [
        "# Download corpus untuk sentiment analysis\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWHhRbz0nJ6J"
      },
      "source": [
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = [] \n",
        "for index, row in tweet_clean.iterrows():\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "  \n",
        "se = pd.Series(listy)\n",
        "tweet_clean['polarity'] = se.values\n",
        "display(tweet_clean.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE7V6DNsMf9p"
      },
      "source": [
        "# Visualisasi Pie Chart\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvLdU2GGoY_d"
      },
      "source": [
        "### **Topic Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSyZiXaiNI3P"
      },
      "source": [
        "# clone tambahan library dari github\n",
        "! git clone https://github.com/machine-learning-ss/tm\n",
        "\n",
        "# Set Data Directory\n",
        "os.chdir('tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7sbfDlWNJUp"
      },
      "source": [
        "import MyLib as TS\n",
        "\n",
        "Tweets = tweet_clean['text']\n",
        "print('Total loaded tweets = {0}'.format(len(Tweets)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPBZxuW0NYoH"
      },
      "source": [
        "n_topics = 4\n",
        "top_topics = 4\n",
        "top_words = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_mznEgLNvK2"
      },
      "source": [
        "# Feature Extraction\n",
        "count_vector = CountVectorizer(token_pattern = r'\\b[a-zA-Z]{3,}\\b') \n",
        "dtm_tf = count_vector.fit_transform(Tweets)\n",
        "tf_terms = count_vector.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1NgY3twNwBB"
      },
      "source": [
        "# Fungsi untuk mencari topic\n",
        "lda_tf = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0).fit(dtm_tf)\n",
        "\n",
        "# Menampilkan Topik\n",
        "vsm_topics = lda_tf.transform(dtm_tf); doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\n",
        "print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\n",
        "plt.hist(np.array(doc_topic), alpha=0.5); plt.show()\n",
        "print('Printing top {0} Topics, with top {1} Words:'.format(top_topics, top_words))\n",
        "TS.print_Topics(lda_tf, tf_terms, top_topics, top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOcvEGgENy0f"
      },
      "source": [
        "# Visualisasi Topic Secara Interaktif\n",
        "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, count_vector) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B07sph-yRgrI"
      },
      "source": [
        "### **Text Network Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Ue_Hm2R-vV"
      },
      "source": [
        "# Pilih teks\n",
        "text = tweet_clean['text']\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj-AW6a3SP3c"
      },
      "source": [
        "# Tokenize\n",
        "text_data = [word_tokenize(i) for i in text]\n",
        "print(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nqqlH4NTgyK"
      },
      "source": [
        "# Membuat fungsi cooccurence\n",
        "def generate_co_occurrence_matrix(corpus):\n",
        "    vocab = set(corpus)\n",
        "    vocab = list(vocab)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    bi_grams = list(bigrams(corpus))\n",
        "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
        " \n",
        "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
        " \n",
        "    for bigram in bigram_freq:\n",
        "        current = bigram[0][1]\n",
        "        previous = bigram[0][0]\n",
        "        count = bigram[1]\n",
        "        pos_current = vocab_index[current]\n",
        "        pos_previous = vocab_index[previous]\n",
        "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
        "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
        " \n",
        "    return co_occurrence_matrix, vocab_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJnRvQEuTwPz"
      },
      "source": [
        "# Membuat Adjacency Matrix\n",
        "data = list(itertools.chain.from_iterable(text_data))\n",
        "matrix, vocab_index = generate_co_occurrence_matrix(data)\n",
        " \n",
        " \n",
        "data_matrix = pd.DataFrame(matrix, index=vocab_index,\n",
        "                             columns=vocab_index)\n",
        "\n",
        "# Show Adjacency Matrix\n",
        "data_matrix.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9zsB-9qT0sA"
      },
      "source": [
        "# Membuat Network dengan Adjacency Matrix\n",
        "G = nx.from_pandas_adjacency(data_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbJuj86QZrwr"
      },
      "source": [
        "# Hitung Degree Centrality dan urutkan dari yang terbesar\n",
        "degree = dict(sorted(nx.degree_centrality(G).items(), key=lambda item: item[1], reverse = True))\n",
        "list_node = list(degree)\n",
        "\n",
        "# Membuat Subraph 50 node dengan Degree Tertinggi\n",
        "sampled_graph = G.subgraph(list_node[1:50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYCBGhCoT5ym"
      },
      "source": [
        "# Seting Ukuran Visualisasi\n",
        "plt.figure(figsize=(10, 10), \n",
        "           facecolor=None)\n",
        "\n",
        "# Visualisasi TNA\n",
        "nx.draw(sampled_graph,\n",
        "        with_labels = True, \n",
        "        node_color='skyblue', \n",
        "        node_size= 1000, \n",
        "        arrowstyle='->',\n",
        "        arrowsize=20, \n",
        "        edge_color='red',\n",
        "        font_size=12,\n",
        "        pos=nx.circular_layout(sampled_graph))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}